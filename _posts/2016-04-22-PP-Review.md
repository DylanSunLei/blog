---
layout:     post
title:      "PP final review"
subtitle:   " Blobfish is not that ugly!"
date:       2016-04-22 12:00:00
author:     "Dylan Sun"

catalog: true
tags:
    - CMU
    - 15418
---




# Lec 1: Why Parallelism

## Speedup

Speedup (using P processors) = execution time (using 1 processor) / execution time (using P processors)

限制 Speedup 的 一个原因是 Communication between processors, therefore, let's put processors next to each other to minimize the cost of communication.

限制speedup的第二个原因是 Imbalance in work assignment. Improving the distribution of work improved speedup.

当拥有超多的 processor时候， the amount of communication is significant compared to computation. Communication costs can dominate in a parallel computation, *severly limiting* speedup.

## Parallel Themes

Theme 1: Parallel Thinking and Designing and writting parallel programs (software)

1. Decomposing work into pieces that can safely be performed in paraleel
2. Assigning work to processors
3. Managing communication/synchronization between the processors so that it does not limit speedup

Theme 2: Parallel computer hardware implementation:how parallel computers work

1. Mechanisms used to implement abstractions efficiently
    - Performance characteristics of implementations
    - Design trade-offs: performance vs convenience vs cost
2. Why we care?
    - Characteristics of the machine really matter (recall speed of communication issues in earlier demos)
    - We care about efficiency and performance.

Theme 3: Thinking about efficiency

- Fast != Efficient
- Code runs faster on a parallel computer, it does not mean it is using the hardware efficiently
    - 2x speedup on 10 processors
- As a programer, make use of provided machine capabilities
- AS A Hardware desisgner's perspective: choosing the right capabilities to pu in system (performance/cost, cost = silicon area? power? etc.)

## Why Parallelism?

从2006年开始， 提高processor的两个办法， increase clock frequency and exploiting instruction-level parallelism(ILP) 都受到了限制， clock frequency 因为过高导致芯片太热而被限制。

ILP idea: independent instructions can be executed simultaneously by a processor without impacting program correctness. One example is superscalar execution, processor dynamically finds independent instructions in an instruction sequence and executes them in parallel (within a core).

即使Processor 每个clock能issue的 instruction 越来越多， superscalar execution 的speedup 也会停止在 4， 增长在减少。

## The power wall

A transistor consumes power contains static power and static powers. Dynamic power = capacitive load * voltatges^2 * frequency. High power = high heat. Maximum allowed frequency of a transistor determined by procesor's core voltage.

## Conclusion: Paralism is the primary way to achieve significantly higher application performance for the forseeable future.

# Lec 2: A Modern Multi-Core Processor

## Parallel Execution

```c
//Taylor expansion:  sin(x) = x - x^3/3! + x^5/5! - x^7/7! + ...
void sinx(int N, int terms, float* x, float* result){
  for (int i = 0; i< N; i++){
    float value = x[i];
    float numer = x[i]*x[i]*x[i];
    int denom = 6; //3!
    int sign = -1;
    
    for(int j = 1;j<terms; j++)
    {
      value+=sign*numer/ denom;
      numer *= x[i]*x[i];
      denom *=(2*j+2) * (2*j+3);
      sign*= -1;
    }
    
    result[i] = value;
  }
}
```

After compiling the above program we get instructions. In a processor abstraction, we have Fetch/Decode, ALU (Execution), and Execution Context. "The processor has to complete a fetch, a decode and an execute for a particular assembly instruction in order to complete the execution of the assembly instruction."

## What does it mean by having more transistors before having more processors?

More transistors = larger cache, smarter out-of-order logic, smarter branch predictor, etc.
More transsitors -> smaller transistors -> higher clock frequencies.
In general, majority of chip transistor used to help a single instruction stream run fast.

## Multi-core era,

- Use increasing transistor count ot add more cores to the processors. Two cores = two fetch/decode, ALU, Execution Context
- Each core will be a little slower (25%) due to the less of fancy controls.
- Speedup 2*0.75 = 1.5


## Parallelism in our Program sinx
However, two cores won't make our code run faster. There's no parallelism in our code.

First solution:  we could split the input into 2 arrays and spawn pthreads to express parallelism.
Second solution: just replace the for loop with a "forall" expression to let the compiler find the optimal value. (Kayvon's fictitious data-parallel language)

### SIMD processing "Single instruction, multiple data"

The basic idea: same instruction broadcast to all ALUs. Executed in parallel on all ALUs.

Amortize cost/complexity of managing an instruction stream across many ALUs.

Another interesting fact: every iteration of the loop do the same thing (no branching, no if..). Therefore we could "amortize cost/complexity of managing an instruction stream across many ALUS"

#### Conditional Execution in SIMD

The code has to have instructions for both path (if x > 8 and else). When executing x > 8, it need to mask(disard) all the x not > 8's lane. In other words, if there's a condtional execution in SIMD, not all ALUs are doing useful work at all the time. Worst case will be 1/#lanes peak perfomance.

## Terminology
- Instruction stream coherence ("coherent execution") 没有conditional execution
- "Divergent execution": a lack of instruction stream coherence

## SIMD execution on modern CPUs
- Instructions are generated by the compiler
    - Parallelism explicitly requested by programmer using intrinsics
    - Parallelism conveyed using parallel language semantics (forall)
    - Parallelism inferred by a dependency analysis of loops (Even the best complers are not great on arbitrary C/C++ code)
- Terminology: 
    - "explicit SIMD" SIMD parallelization is performed at *compile time*.
    - "Implicit SIMD" Compiler generates a scalar binary (scalar instructions)
        - execute (my_function, N) //execute my_function N times
        - In other words, the interface to the hardware itself is data-parallel
SIMD width of most modern GPU ranges from 8 to 32
- Divergence can be a big issue. (poorly writen code might execute at 1/32 the peak capability of the machine)


## Assigning memory

### Terminology
- Memory Latency
    - The amout of time for a memory request from a processor to be serviced by the memory system
    - e.g. 100 cycles
- Memory bandwidth
    - The rate at which the memory system can provide data to a processor
    - 20 GB/s
- Stall
    - A processor "stall" when it cannot run the next instruction in an instruction stream becaause of a dependency on a previous instruction.
    - Accessing memory is a major source of stalls
- Memory access times ~ 100's of cycles
    - Memory access time is a measure of latency

Cache reduces length of stalls. Cache reduces memory access latency. Cache also provide high bandwidth data transfer to CPU.

### Prefetching reduces stalls (hides latency)

Prefetching reduces stalls since data is resident in cache when accessed.
Prefetching can reduce performance if the guess is wrong.

### Multi-threading reduces stalls

Idea: interleave processing of multiple threads on the same core ot hide stalls.

Like prefetching, multi-threading is a latency hiding, not a latencn reducing technique.

#### Thoughput computing trade-off

Use multiple thread to hide latency will potentially increase the time to complete work by any one thread. However, the overall system throughput will be increased. It's good for thoughput-oriented system.

<img src="http://15418.courses.cs.cmu.edu/spring2016content/lectures/02_basicarch/images/slide_053.jpg">

### Storing Execution contextss (Context Storage, or L1 Cache)

Many small contexts vs 4 large contexts: high latency hiding ability vs low latency hiding ability














    









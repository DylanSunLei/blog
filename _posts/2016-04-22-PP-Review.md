
---
layout:     post
title:      "PP final review"
subtitle:   " Blobfish is not that ugly!"
date:       2016-04-22 12:00:00
author:     "Dylan Sun"

catalog: true
tags:
    - CMU
    - 15418
---




# Lec 1: Why Parallelism

## Speedup

Speedup (using P processors) = execution time (using 1 processor) / execution time (using P processors)

限制 Speedup 的 一个原因是 Communication between processors, therefore, let's put processors next to each other to minimize the cost of communication.

限制speedup的第二个原因是 Imbalance in work assignment. Improving the distribution of work improved speedup.

当拥有超多的 processor时候， the amount of communication is significant compared to computation. Communication costs can dominate in a parallel computation, *severly limiting* speedup.

## Parallel Themes

Theme 1: Parallel Thinking and Designing and writting parallel programs (software)

1. Decomposing work into pieces that can safely be performed in paraleel
2. Assigning work to processors
3. Managing communication/synchronization between the processors so that it does not limit speedup

Theme 2: Parallel computer hardware implementation:how parallel computers work

1. Mechanisms used to implement abstractions efficiently
    - Performance characteristics of implementations
    - Design trade-offs: performance vs convenience vs cost
2. Why we care?
    - Characteristics of the machine really matter (recall speed of communication issues in earlier demos)
    - We care about efficiency and performance.

Theme 3: Thinking about efficiency

- Fast != Efficient
- Code runs faster on a parallel computer, it does not mean it is using the hardware efficiently
    - 2x speedup on 10 processors
- As a programer, make use of provided machine capabilities
- AS A Hardware desisgner's perspective: choosing the right capabilities to pu in system (performance/cost, cost = silicon area? power? etc.)

## Why Parallelism?

从2006年开始， 提高processor的两个办法， increase clock frequency and exploiting instruction-level parallelism(ILP) 都受到了限制， clock frequency 因为过高导致芯片太热而被限制。

ILP idea: independent instructions can be executed simultaneously by a processor without impacting program correctness. One example is superscalar execution, processor dynamically finds independent instructions in an instruction sequence and executes them in parallel.

即使Processor 每个clock能issue的 instruction 越来越多， superscalar execution 的speedup 也会停止在 4， 增长在减少。

## The power wall

A transistor consumes power contains static power and static powers. Dynamic power = capacitive load * voltatges^2 * frequency. High power = high heat. Maximum allowed frequency of a transistor determined by procesor's core voltage.

## Conclusion: Paralism is the primary way to achieve significantly higher application performance for the forseeable future.

# Lec 2: A Modern Multi-Core Processor

# Parallel Execution

```c
//Taylor expansion:  sin(x) = x - x^3/3! + x^5/5! - x^7/7! + ...
void sinx(int N, int terms, float* x, float* result){
  for (int i = 0; i< N; i++){
    float value = x[i];
    float numer = x[i]*x[i]*x[i];
    int denom = 6; //3!
    int sign = -1;
    
    for(int j = 1;j<terms; j++)
    {
      value+=sign*numer/ denom;
      numer *= x[i]*x[i];
      denom *=(2*j+2) * (2*j+3);
      sign*= -1;
    }
    
    result[i] = value;
  }
}
```

After compiling the above program we get instructions. In a processor abstraction, we have Fetch/Decode, ALU (Execution), and Execution Context. "The processor has to complete a fetch, a decode and an execute for a particular assembly instruction in order to complete the execution of the assembly instruction."

## What does it mean by having more transistors before having more processors?

More transistors = larger cache, smarter out-of-order logic, smarter branch predictor, etc.
More transsitors -> smaller transistors -> higher clock frequencies.
In general, majority of chip transistor used to help a single instruction stream run fast.

## Multi-core era,

- Use increasing transistor count ot add more cores to the processors. Two cores = two fetch/decode, ALU, Execution Context
- Each core will be a little slower (25%) due to the less of fancy controls.
- Speedup 2*0.75 = 1.5


## Parallelism in our Program sinx
However, two cores won't make our code run faster. There's no parallelism in our code.

First solution:  we could split the input into 2 arrays and spawn pthreads to express parallelism.
Second solution: just replace the for loop with a "forall" expression to let the compiler find the optimal value. (Kayvon's fictitious data-parallel language)

### SIMD processing "Single instruction, multiple data"

The basic idea: same instruction broadcast to all ALUs. Executed in parallel on all ALUs.

Amortize cost/complexity of managing an instruction stream across many ALUs.

Another interesting fact: every iteration of the loop do the same thing (no branching, no if..). Therefore we could "amortize cost/complexity of managing an instruction stream across many ALUS"

#### Conditional Execution in SIMD

The code has to have instructions for both path (if x > 8 and else). When executing x > 8, it need to mask(disard) all the x not > 8's lane. In other words, if there's a condtional execution in SIMD, not all ALUs are doing useful work at all the time. Worst case will be 1/#lanes peak perfomance.

## Terminology
- Instruction stream coherence ("coherent execution")
- ""










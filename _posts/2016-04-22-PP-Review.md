---
layout:     post
title:      "PP final review"
subtitle:   " Blobfish is not that ugly!"
date:       2016-04-22 12:00:00
author:     "Dylan Sun"

catalog: true
tags:
    - CMU
    - 15418
---




# Lec 1: Why Parallelism

## Speedup

Speedup (using P processors) = execution time (using 1 processor) / execution time (using P processors)

限制 Speedup 的 一个原因是 Communication between processors, therefore, let's put processors next to each other to minimize the cost of communication.

限制speedup的第二个原因是 Imbalance in work assignment. Improving the distribution of work improved speedup.

当拥有超多的 processor时候， the amount of communication is significant compared to computation. Communication costs can dominate in a parallel computation, *severly limiting* speedup.

## Parallel Themes

Theme 1: Parallel Thinking and Designing and writting parallel programs (software)

1. Decomposing work into pieces that can safely be performed in paraleel
2. Assigning work to processors
3. Managing communication/synchronization between the processors so that it does not limit speedup

Theme 2: Parallel computer hardware implementation:how parallel computers work

1. Mechanisms used to implement abstractions efficiently
    - Performance characteristics of implementations
    - Design trade-offs: performance vs convenience vs cost
2. Why we care?
    - Characteristics of the machine really matter (recall speed of communication issues in earlier demos)
    - We care about efficiency and performance.

Theme 3: Thinking about efficiency

- Fast != Efficient
- Code runs faster on a parallel computer, it does not mean it is using the hardware efficiently
    - 2x speedup on 10 processors
- As a programer, make use of provided machine capabilities
- AS A Hardware desisgner's perspective: choosing the right capabilities to pu in system (performance/cost, cost = silicon area? power? etc.)

## Why Parallelism?

从2006年开始， 提高processor的两个办法， increase clock frequency and exploiting instruction-level parallelism(ILP) 都受到了限制， clock frequency 因为过高导致芯片太热而被限制。

ILP idea: independent instructions can be executed simultaneously by a processor without impacting program correctness. One example is superscalar execution, processor dynamically finds independent instructions in an instruction sequence and executes them in parallel (within a core).

即使Processor 每个clock能issue的 instruction 越来越多， superscalar execution 的speedup 也会停止在 4， 增长在减少。

## The power wall

A transistor consumes power contains static power and static powers. Dynamic power = capacitive load * voltatges^2 * frequency. High power = high heat. Maximum allowed frequency of a transistor determined by procesor's core voltage.

## Conclusion: Paralism is the primary way to achieve significantly higher application performance for the forseeable future.

# Lec 2: A Modern Multi-Core Processor

## Parallel Execution

```c
//Taylor expansion:  sin(x) = x - x^3/3! + x^5/5! - x^7/7! + ...
void sinx(int N, int terms, float* x, float* result){
  for (int i = 0; i< N; i++){
    float value = x[i];
    float numer = x[i]*x[i]*x[i];
    int denom = 6; //3!
    int sign = -1;
    
    for(int j = 1;j<terms; j++)
    {
      value+=sign*numer/ denom;
      numer *= x[i]*x[i];
      denom *=(2*j+2) * (2*j+3);
      sign*= -1;
    }
    
    result[i] = value;
  }
}
```

After compiling the above program we get instructions. In a processor abstraction, we have Fetch/Decode, ALU (Execution), and Execution Context. "The processor has to complete a fetch, a decode and an execute for a particular assembly instruction in order to complete the execution of the assembly instruction."

## What does it mean by having more transistors before having more processors?

More transistors = larger cache, smarter out-of-order logic, smarter branch predictor, etc.
More transsitors -> smaller transistors -> higher clock frequencies.
In general, majority of chip transistor used to help a single instruction stream run fast.

## Multi-core era,

- Use increasing transistor count ot add more cores to the processors. Two cores = two fetch/decode, ALU, Execution Context
- Each core will be a little slower (25%) due to the less of fancy controls.
- Speedup 2*0.75 = 1.5


## Parallelism in our Program sinx
However, two cores won't make our code run faster. There's no parallelism in our code.

First solution:  we could split the input into 2 arrays and spawn pthreads to express parallelism.
Second solution: just replace the for loop with a "forall" expression to let the compiler find the optimal value. (Kayvon's fictitious data-parallel language)

### SIMD processing "Single instruction, multiple data"

The basic idea: same instruction broadcast to all ALUs. Executed in parallel on all ALUs.

Amortize cost/complexity of managing an instruction stream across many ALUs.

Another interesting fact: every iteration of the loop do the same thing (no branching, no if..). Therefore we could "amortize cost/complexity of managing an instruction stream across many ALUS"

#### Conditional Execution in SIMD

The code has to have instructions for both path (if x > 8 and else). When executing x > 8, it need to mask(disard) all the x not > 8's lane. In other words, if there's a condtional execution in SIMD, not all ALUs are doing useful work at all the time. Worst case will be 1/#lanes peak perfomance.

## Terminology
- Instruction stream coherence ("coherent execution") 没有conditional execution
- "Divergent execution": a lack of instruction stream coherence

## SIMD execution on modern CPUs
- Instructions are generated by the compiler
    - Parallelism explicitly requested by programmer using intrinsics
    - Parallelism conveyed using parallel language semantics (forall)
    - Parallelism inferred by a dependency analysis of loops (Even the best complers are not great on arbitrary C/C++ code)
- Terminology: 
    - "explicit SIMD" SIMD parallelization is performed at *compile time*.
    - "Implicit SIMD" Compiler generates a scalar binary (scalar instructions)
        - execute (my_function, N) //execute my_function N times
        - In other words, the interface to the hardware itself is data-parallel
SIMD width of most modern GPU ranges from 8 to 32
- Divergence can be a big issue. (poorly writen code might execute at 1/32 the peak capability of the machine)


## Assigning memory

### Terminology
- Memory Latency
    - The amout of time for a memory request from a processor to be serviced by the memory system
    - e.g. 100 cycles
- Memory bandwidth
    - The rate at which the memory system can provide data to a processor
    - 20 GB/s
- Stall
    - A processor "stall" when it cannot run the next instruction in an instruction stream becaause of a dependency on a previous instruction.
    - Accessing memory is a major source of stalls
- Memory access times ~ 100's of cycles
    - Memory access time is a measure of latency

Cache reduces length of stalls. Cache reduces memory access latency. Cache also provide high bandwidth data transfer to CPU.

### Prefetching reduces stalls (hides latency)

Prefetching reduces stalls since data is resident in cache when accessed.
Prefetching can reduce performance if the guess is wrong.

### Multi-threading reduces stalls

Idea: interleave processing of multiple threads on the same core ot hide stalls.

Like prefetching, multi-threading is a latency hiding, not a latencn reducing technique.

#### Thoughput computing trade-off

Use multiple thread to hide latency will potentially increase the time to complete work by any one thread. However, the overall system throughput will be increased. It's good for thoughput-oriented system.

<img src="http://15418.courses.cs.cmu.edu/spring2016content/lectures/02_basicarch/images/slide_053.jpg">

### Storing Execution contextss (Context Storage, or L1 Cache)

Many small contexts vs 4 large contexts: high latency hiding ability vs low latency hiding ability.

### Hardware-supported multi-threading

- Core manages execution contexts for multiple threads
    - Processor choose which thread to run, not the operating system
    - Core still has the same number of ALU resoures: multi-threading only helps use them more efficiently in the face of high-latency operations like memory access.
- Interleaved multi-threading (temporal multi-threading)
    - each clock, the core choose a thread, and run instruction from the thread on the ALUs.
- Simultaneous mlti-threading (SMT)
    - Each clock, core choose instructions from multiple threads to run on ALUs.
    - Extension of superscalar CPU design.
    - Example: Intel Hyper-threading (2 threads per core)

## Multi-threading summary
- Benefit: use a core's ALU resources more efficiently
    - Hide memory latency (ALU 一直在干活， 不会因为memort access time 而 idle)
    - Fill multople functional units of superscalar architecture (when one thread has insufficient ILP)
- Costs
    - Requires additional storage for thread contexts.
    - Increases run time of any single thread (usually we care about thoughput)
    - Requires additional independent work in a program (more independent work than ALUs) 需要程序本身更多的独立工作,很可能导致总数更多的工作
    - Relies heavily on memory bandwidth
        - More threads -> larger working set -> less cache space per thread
        - May go to memory more often, but can hide the latency


### GPU: Extreme throughput-oriented processors

16 SIMD function units but it operate on 32 pieces of data at a time (called "warps")

Warp=thread issuing 32-wide vector instructions.

Up to 48 warps are simultaneously interleaved.

Over 1500 elements can be processed concurrently by a core.  (48*32=1532)

<img src="http://15418.courses.cs.cmu.edu/spring2016content/lectures/02_basicarch/images/slide_060.jpg">

### CPU vs. GPU memory hierarchies

CPU: big caches (L3 8 MB), few threads, modest memory BW (25 GB/s). Rely mainly on caches and prefetching.

GPU: small caches (L2 678 KB), many threads, huge memory BW (177 GB/s). Rely mainly on multi-threading.


### 练习题

<img src="http://15418.courses.cs.cmu.edu/spring2016content/lectures/02_basicarch/images/slide_065.jpg">

"GTX480 has 15 cores and 32 ALUs each core, so totally (15*32=) 480 ALUs. When we make full use of ALUs, we can have 480 instructions run per cycle. As Frequency is 1.2 GHz, we can have (480 * 1.2 * 10^9 =) 5.76e11 multiplications per second. Each multiplication involves (3 int's =) 12 bytes, so total memory bandwidth needs to be (5.76e11 * 12 * 1024^-3=) 6.4 TB/s in order to support that many multiplications. Here we only have 177GB/s, so efficiency is only about (177 / 6400 =) 2.7%."

Conclusion: Bandwidth limited! If processor request data at too high a rate, the memory systemm cannot keep up.

Bandwidth is a critical resource.The performanct parallel programs will:
- Organize computation to fetch data from memory less often (Reuse data/locality, share data across threads/ inter-thread cooperation)
- Request data less often (do more arithmetic)
    - Arithmetic intensity - ratio of math operations to data access operations in an instruction stream
    - Programs must have high arithmetic intensity to utilize modern processors efficiently.

### Summary

- 三个现代processor的 idea：
    - 多个processing cores （更简单的core）
    - Amortize instruction stream processing over many ALUs (SIMD) 用一点点代价来增加compute capability
    - 用多线程使processing resource 更有效率 （hide latency， fill all available resources）
- 由于现代chip的高arithmetic capability，很多parallel applications 都是 bandwidth bound
- GPU push the concepts of arthmetic intensity to extreme.

### Quad-core processor, two way multi-threading per core (一个core 两个execution context， 一个chip 最多8个threads)， up to two instructions per clock per core (one of those instructions is 8 wide SIMD) (superscalar execution feature)

<img src="http://15418.courses.cs.cmu.edu/spring2016content/lectures/02_basicarch/images/slide_079.jpg">



# Lec3: Parallel Programming Abstractions (ISPC)

Today's Theme: abstraction vs implementation (conflating these two is a common cause for confusion)


## ISPC: Intel SPMD Program Compiler

Single program multiple data

```c
#include "sinx_ispc.h"

int N = 1024;
int terms = 5;
float* x = new float[N];
float* result = new float[N];
//initialize x here;
//execute ISPC code
sinx(N, terms, x, result);
```
SPMD programming abstraction: Call to ISPC fucntion (sinx) spawns "gang" of ISPC "program instances". All instances run ISPC code concurrently. Upon return, all instances have completed. 

Assume programCount=8 in this case, sinx() ISPC code is run in 8-wide SIMD fashion.

sinx in ISPC: interleaved assignment of array elements to program instances.

ISPC Keywords:
- programCount: number of simultaneously executing instances in the gang (uniform value)
- prgramIndex: id of the current instance in the gang. (a non-uniform value: "varying")
- uniform: a type modifier. All instances have the same value for this variable. Its use is purely an optimization.Not needed for correctness.

<img src="http://15418.courses.cs.cmu.edu/spring2016content/lectures/03_progabstractions/images/slide_007.jpg">

programCount is the number of instances in a gang.

### ISPC implements the gang abstraction using SIMD instructions

Number of instances in a gang is the SIMD width of the hardware (or a small multiple of SIMD width) (This is also called the size of a gang)


### Two version of sinx implementation in ISPC: Interleaved vs Blocked assignment

Two different schedule. Interleaved assignment is preferred since it has better spatial locality.

Also, blocked assignment would need "_mm_i32gather" instruction (gather) to implement. Gather instruction is a more complex and more costly SIMD instruction. On the other hadn, interleaved assignment will need "_mm_load_ps1".

### "foreach" raising level of abstraction

"foreach": key ISPC language construct
- foreach delcares parallel loop iterations. Programmer says: these are the iterations the instances in a gang cooperatively must perform.
- ISPC implementation assigns iterations to program instances gang. (by default, it's a static interleaved assignment, but the abstraactions permits a different assignment)

### ISPC: abstraction vs implementation

Single program, multiple data (SPMD) programming model
- Programmer thinks: running a gang is spawing programCount logical instruction streams (each with a different value of programIndex)
- This is *programming abstraction*. Program is written in terms of this abstraction.

Single instruction, multiple data (SIMD) implementation
- ISPC compiler emits vector instructions (SSE4 or AVX) that carry out the logic performed by a ISPC gang.
- ISPC compiler handles mapping of conditional control flow to vector instructions (by masking vector lanes, etc)

Semantic of ISPC can be tricky
- SPMD abstraction + uniform values

### ISPC discussion: sum "reduction"
uniform 的 variable 不能 +=

### ISPC tasks: another ISPC abstraction 

- The ISPC gang abstracition is implemented by SIMD instructions on one core.
- The code will have executed on only one of the four cores of the GHC machines.
- ISPC contains another abstraction: a "task" that is used to achieve multi-core execution.


### Three models of communication (abstractions vs HW implementation)
1. Shared address space
    - Threads communicate by reading/writting to shared variables
    - Synchronization primitives are also shared variables: locks
    - HW Implementation
        - "Dance-hall" and Interconnect
        - Symemtric (share-memory) multi-processor (SMP): uniform memory access time: cost of accessing an uncached memory address is the same for all processors.
            - Problem with preserving uniform access time: scalability
            - Uniformly bad
        - Non-uniform memory access (NUMA)
            - More scalable
            - local memory have low latency access and high bandwidth
            - Cost is increased programmer effort for performance tuning: finding, exploiting locality will be crucial.
    - Summary
        - requires hardware support to implement efficiently
        - Even with NUMA, costly to scale (one of the reasons why super computers are expensive)
2. Message passing
3. Data Parallel














    








